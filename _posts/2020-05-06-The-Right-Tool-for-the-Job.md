---
layout: post
comments: true
title:  "[논문 뽀개기] The Right Tool for the Job: Matching Model and Instance Complexities"
date:   2020-05-06 15:01:19
author: devsaka
categories: AI
tags:
  - Machine Learning
  - NLP
  - 논문
  - Paper
image: https://steamcdn-a.akamaihd.net/steam/apps/502130/ss_7cf6c50fcfaa607533d06ca6b2a29080181a8b64.1920x1080.jpg?t=1581833942
---

```
정확도를 유지하면서 학습 시간을 최대한 줄여보자!
```

## 읽기 전에

언제나 좋은 글을 소개해주시는 [월간 자연어 처리](https://www.facebook.com/monthly.nlp/posts/232068938224978) 페이지에 감사합니다.

NLP에서 BERT, GPT와 같은 사전 훈련 언어 모델(Pre-trained Language Model, PLM)들이 레이어를 쌓으면 쌓을수록 정확도가 올라간다는 것을 보여준 이후로, 학습 시간도 그만큼 많이 길어졌다. 논문만 봐도 **이 모델을 학습하는데 7일이 걸렸습니다** 라는 내용을 어렵지 않게 찾아볼 수 있는 정도가 되었다. 

사실 레이어를 많이 쌓으면 쌓을수록 정확도가 올라간다는 것은, 작은 모델이 잘 분류하지 못하는 복잡한 문장을 상대적으로 잘 잡아낸다는 뜻이고, 이걸 좀 다른 시각으로 본다면 복잡하지 않은 문장은 작은 모델로도 잘 잡아낸다 라는 이야기로 볼 수도 있다. 예를 들어 

```
각본을 12살 짜리 꼬마가 쓴건지, 아카데미 수상자가 쓴건지 모르겠네요
```
해당 문장이 긍정인지 부정인지 알기 위해서는 '12살 짜리 꼬마가 썼다' 와 '수상자'에 대한 관계를 파악해야하기 때문에 쉽게 분류할 수 없다 하지만

```
이 영화 재밌네요
```
이 문장은 BERT 모델이 아니라 Vanilla RNN(진짜 기본적인 RNN)에 넣어도 긍정으로 잡아낸다. 여기서 착안해 이 논문은 어려운 인스턴스들은 많은 레이어를 거치게 하고, 저층 레이어에서 분류가 가능한 인스턴스는 빠르게 결과를 내보내서 정확도와 시간의 Trade off를 일정 부분 해결할 수 있겠다는 내용을 작성했다.

## 참조한 사이트

- [월간 자연어 처리](https://www.facebook.com/monthly.nlp/posts/232068938224978)
- [논문](https://arxiv.org/pdf/2004.07453.pdf?fbclid=IwAR0-THZxTeC5OCr6p502rHjxTraK8Rv7Jzjrl6BEOi7JWWtFeGDgtW1DwB4)
- [해당 논문의 깃헙](https://github.com/allenai/sledgehammer?fbclid=IwAR2pWJtbfCCQv2DA_V2nm0jjilnYycJg-aMNwI3py-rOqNXbbVukOctZe0I)


## 요약
- NLP 모델이 커짐에 따라, 훈련된 모델을 실행하는 것은 돈이 많이 들고, 환경을 구축하는 비용이 상당한 계산 리소스가 요구됨
  - 기본적으로 그래픽카드가 엄청 많이 필요하니까 환경 구축 비용도 어마무시하고, 이렇게 많이 써도 며칠씩 훈련을 돌리니 비용이 어마무시하다고..

- 주어진 추론 예산(budget)을 더 잘 쓰기 위해서, 문맥적 표현 미세 조정 수정을 제안하는데, 여기서 추론하는 동안 간단한 인스턴스는 신경망 계산에서 early 'exit' 시키고 어려운 인스턴스는 late 'exit' 시키는 것을 따른다.
  - 간단한 인스턴스는 빠르게 결과값을 내고, 복잡한 인스턴스는 정확도를 위해서 좀 더 늦게 결과값을 낸다고 이해하면 된다.

- 이걸 달성하기 위해서, BERT의 레이어들에 분류기를 추가하고 early exit 결정을 위해 이 분류기들의 Confidence score를 교정한다.

- 이 제안을 테스트해보기 위해서 두 가지 과제에서 다섯 가지 데이터 셋을 사용했다.
  - 세 개의 텍스트 분류 과제
  - 두 개의 자연어 추론 벤치마크

- 제안한 방법들이 거의 모든 과제에서 흥미로운 속도와 정확도간의 트레이트 오프를 보여주며, state of the art 모델들의 정확도를 유지하면서 다섯 배나 빠른 모델을 만들어냈다. 이 방법들은 BERT 모델과 비교해서 추가적인 시간이나 파라미터가 필요하지 않다.

- 마지막으로 이 방법은 다양한 수준의 효율성에서 여러 모델의 재훈련에 대해 비용이 많이 들어가는 필요성을 해소시키는데, 사용자가 추론 시간에 단일 변수를 설정하여 단일 훈련 모델을 사용하여 속도/정확도 트레이드 오프를 제어할 수 있도록 허용한다.

